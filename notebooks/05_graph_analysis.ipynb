{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05: Graph Analysis\n",
    "\n",
    "This notebook performs comprehensive network analysis on the brain connectome graph. It computes centrality measures to identify hub regions, detects community structure, and calculates global network properties. These metrics reveal the functional organization and hierarchical structure of the brain.\n",
    "\n",
    "Key steps:\n",
    "1. Load the connectome graph from Notebook 04\n",
    "2. Compute centrality measures (betweenness, degree, pagerank, clustering, strength)\n",
    "3. Create metrics DataFrame and identify TOP 15 HUB REGIONS\n",
    "4. Save hub regions to CSV\n",
    "5. Detect communities using greedy modularity optimization\n",
    "6. Calculate global network properties\n",
    "7. Save communities to JSON\n",
    "8. Create summary visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "# networkx: graph algorithms and analysis\n",
    "# numpy: numerical computing\n",
    "# pandas: data manipulation\n",
    "# json: save/load structured data\n",
    "# pathlib: cross-platform file handling\n",
    "# community: modularity optimization for community detection\n",
    "# collections: data structures\n",
    "# matplotlib: visualization\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "import community\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All required libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Random seed set to 42 for reproducibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration and set up directories\n",
    "notebook_dir = Path.cwd()\n",
    "project_dir = notebook_dir.parent\n",
    "data_dir = project_dir / 'data'\n",
    "results_dir = project_dir / 'results'\n",
    "figures_dir = project_dir / 'figures'\n",
    "\n",
    "# Load config\n",
    "config_path = data_dir / 'config.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "n_regions = config['n_regions']\n",
    "\n",
    "print(f\"Configuration loaded from {config_path}\")\n",
    "print(f\"Number of brain regions: {n_regions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the connectome graph from Notebook 04\n",
    "# GraphML format preserves all node/edge properties\n",
    "print(\"Loading connectome graph from Notebook 04...\")\n",
    "\n",
    "graph_path = results_dir / 'connectome_graph.graphml'\n",
    "G = nx.read_graphml(graph_path)\n",
    "\n",
    "# Convert node labels from strings to integers (GraphML stores as strings)\n",
    "G = nx.convert_node_labels_to_integers(G)\n",
    "\n",
    "print(f\"Graph loaded from {graph_path}\")\n",
    "print(f\"Graph properties:\")\n",
    "print(f\"  Nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  Edges: {G.number_of_edges()}\")\n",
    "print(f\"  Density: {nx.density(G):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute betweenness centrality for all nodes\n",
    "# Betweenness centrality measures how often a node lies on shortest paths between other nodes\n",
    "# High betweenness = node is important for communication between different network parts\n",
    "# Interpretation: identifies 'hub' regions that integrate information\n",
    "\n",
    "print(\"Computing betweenness centrality...\")\n",
    "print(\"  Definition: How often a node lies on shortest paths between other nodes\")\n",
    "print(\"  Interpretation: High values indicate integration hubs\")\n",
    "\n",
    "betweenness = nx.betweenness_centrality(G, weight='weight')\n",
    "betweenness = {int(k): v for k, v in betweenness.items()}\n",
    "\n",
    "print(f\"  Completed. Range: [{min(betweenness.values()):.4f}, {max(betweenness.values()):.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute degree centrality for all nodes\n",
    "# Degree = number of direct connections to other nodes\n",
    "# Simple measure: more connections = more central\n",
    "\n",
    "print(\"\\nComputing degree centrality...\")\n",
    "print(\"  Definition: Number of direct connections\")\n",
    "print(\"  Interpretation: Nodes with many connections\")\n",
    "\n",
    "degree = dict(G.degree())\n",
    "degree_norm = {k: v / (n_regions - 1) for k, v in degree.items()}  # Normalize by max possible\n",
    "\n",
    "print(f\"  Completed. Range: {min(degree.values())}-{max(degree.values())} connections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PageRank centrality\n",
    "# Extends degree centrality by weighing connections to high-centrality nodes\n",
    "# Algorithm from Google's search ranking\n",
    "# High PageRank = connected to other important nodes\n",
    "\n",
    "print(\"\\nComputing PageRank centrality...\")\n",
    "print(\"  Definition: Weighted by importance of connected nodes\")\n",
    "print(\"  Interpretation: Identifies nodes in prominent positions\")\n",
    "\n",
    "pagerank = nx.pagerank(G, weight='weight')\n",
    "pagerank = {int(k): v for k, v in pagerank.items()}\n",
    "\n",
    "print(f\"  Completed. Range: [{min(pagerank.values()):.4f}, {max(pagerank.values()):.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute clustering coefficient for all nodes\n",
    "# Clustering coefficient measures how connected a node's neighbors are to each other\n",
    "# High clustering = node is part of a tight cluster (local specialization)\n",
    "# Low clustering = node connects disparate groups (bridging function)\n",
    "\n",
    "print(\"\\nComputing clustering coefficient...\")\n",
    "print(\"  Definition: How tightly connected a node's neighbors are\")\n",
    "print(\"  Interpretation: High=local processing, Low=bridging between regions\")\n",
    "\n",
    "clustering = nx.clustering(G, weight='weight')\n",
    "clustering = {int(k): v for k, v in clustering.items()}\n",
    "\n",
    "print(f\"  Completed. Range: [{min(clustering.values()):.4f}, {max(clustering.values()):.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weighted degree (strength) for all nodes\n",
    "# Strength = sum of edge weights connected to a node\n",
    "# Unlike degree (counts edges), strength considers connection intensity\n",
    "# High strength = strongly connected (not just many connections)\n",
    "\n",
    "print(\"\\nComputing weighted degree (strength)...\")\n",
    "print(\"  Definition: Sum of weights of connected edges\")\n",
    "print(\"  Interpretation: Total functional coupling strength\")\n",
    "\n",
    "strength = {}\n",
    "for node in G.nodes():\n",
    "    # Sum all edge weights connected to this node\n",
    "    total_weight = sum([G[node][neighbor]['weight'] for neighbor in G.neighbors(node)])\n",
    "    strength[node] = total_weight\n",
    "\n",
    "print(f\"  Completed. Range: [{min(strength.values()):.4f}, {max(strength.values()):.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics DataFrame combining all centrality measures\n",
    "# Each row = one brain region\n",
    "# Columns = different centrality metrics\n",
    "\n",
    "print(\"\\nCombining metrics into DataFrame...\")\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'region_id': list(range(n_regions)),\n",
    "    'degree': [degree.get(i, 0) for i in range(n_regions)],\n",
    "    'betweenness': [betweenness.get(i, 0) for i in range(n_regions)],\n",
    "    'pagerank': [pagerank.get(i, 0) for i in range(n_regions)],\n",
    "    'clustering': [clustering.get(i, 0) for i in range(n_regions)],\n",
    "    'strength': [strength.get(i, 0) for i in range(n_regions)]\n",
    "})\n",
    "\n",
    "# Normalize metrics to 0-1 range for easier comparison\n",
    "# This allows combining metrics from different scales\n",
    "for col in ['degree', 'betweenness', 'pagerank', 'clustering', 'strength']:\n",
    "    min_val = metrics_df[col].min()\n",
    "    max_val = metrics_df[col].max()\n",
    "    if max_val > min_val:\n",
    "        metrics_df[f'{col}_norm'] = (metrics_df[col] - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        metrics_df[f'{col}_norm'] = 0\n",
    "\n",
    "# Calculate composite hub score (average of normalized metrics)\n",
    "# Identifies regions that are central by multiple measures\n",
    "norm_cols = [col for col in metrics_df.columns if col.endswith('_norm')]\n",
    "metrics_df['hub_score'] = metrics_df[norm_cols].mean(axis=1)\n",
    "\n",
    "# Sort by hub score to identify top hubs\n",
    "metrics_df_sorted = metrics_df.sort_values('hub_score', ascending=False)\n",
    "\n",
    "print(f\"Metrics DataFrame created\")\n",
    "print(f\"  Shape: {metrics_df.shape}\")\n",
    "print(f\"  Columns: {list(metrics_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and display TOP 15 HUB REGIONS\n",
    "# These are the most central regions in the brain connectome\n",
    "\n",
    "n_top_hubs = 15\n",
    "top_hubs = metrics_df_sorted.head(n_top_hubs).copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TOP 15 HUB REGIONS IN BRAIN CONNECTOME\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nThese regions have high centrality across multiple measures and serve as network hubs\\n\")\n",
    "\n",
    "# Create formatted table\n",
    "display_cols = ['region_id', 'degree', 'betweenness', 'pagerank', 'clustering', 'strength', 'hub_score']\n",
    "print(f\"{'Rank':<6} {'Region':<10} {'Degree':<10} {'Between':<12} {'PageRank':<12} {'Cluster':<10} {'Strength':<12} {'Hub Score':<12}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for rank, (idx, row) in enumerate(top_hubs.iterrows(), 1):\n",
    "    print(f\"{rank:<6} {int(row['region_id']):<10} {int(row['degree']):<10} \"\n",
    "          f\"{row['betweenness']:<12.4f} {row['pagerank']:<12.4f} {row['clustering']:<10.4f} \"\n",
    "          f\"{row['strength']:<12.4f} {row['hub_score']:<12.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hub regions to CSV for reference and downstream analysis\n",
    "# This table can be used in other analyses or publications\n",
    "\n",
    "hub_output_path = results_dir / 'hub_regions.csv'\n",
    "top_hubs_export = top_hubs[['region_id', 'degree', 'betweenness', 'pagerank', 'clustering', 'strength', 'hub_score']].copy()\n",
    "top_hubs_export['rank'] = range(1, n_top_hubs + 1)\n",
    "top_hubs_export = top_hubs_export[['rank', 'region_id', 'degree', 'betweenness', 'pagerank', 'clustering', 'strength', 'hub_score']]\n",
    "\n",
    "top_hubs_export.to_csv(hub_output_path, index=False)\n",
    "\n",
    "print(f\"\\nHub regions saved to {hub_output_path}\")\n",
    "\n",
    "# Also save full metrics table\n",
    "full_metrics_path = results_dir / 'all_region_metrics.csv'\n",
    "metrics_export = metrics_df_sorted[['region_id', 'degree', 'betweenness', 'pagerank', 'clustering', 'strength', 'hub_score']].reset_index(drop=True)\n",
    "metrics_export['rank'] = range(1, len(metrics_export) + 1)\n",
    "metrics_export = metrics_export[['rank', 'region_id', 'degree', 'betweenness', 'pagerank', 'clustering', 'strength', 'hub_score']]\n",
    "\n",
    "metrics_export.to_csv(full_metrics_path, index=False)\n",
    "print(f\"Full metrics saved to {full_metrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect community structure using greedy modularity optimization\n",
    "# Communities are groups of regions that are more densely connected to each other than to other regions\n",
    "# Modularity = measure of how well-defined the communities are\n",
    "# Range: -1 to 1 (1 = perfect communities, 0 = random network)\n",
    "\n",
    "print(\"\\nDetecting community structure...\")\n",
    "print(\"  Method: Greedy modularity optimization\")\n",
    "print(\"  Communities = densely connected subgraphs\")\n",
    "\n",
    "# Compute best partition using greedy algorithm\n",
    "partition = community.best_partition(G, weight='weight')\n",
    "\n",
    "# Convert to list of communities\n",
    "communities_dict = defaultdict(list)\n",
    "for node, community_id in partition.items():\n",
    "    communities_dict[community_id].append(int(node))\n",
    "\n",
    "# Calculate modularity\n",
    "modularity = community.modularity(partition, G, weight='weight')\n",
    "\n",
    "n_communities = len(communities_dict)\n",
    "print(f\"\\nCommunity Detection Results:\")\n",
    "print(f\"  Number of communities: {n_communities}\")\n",
    "print(f\"  Modularity: {modularity:.4f}\")\n",
    "print(f\"    (Ranges 0-1: higher = better communities)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print community information\n",
    "# Shows which regions belong to which community\n",
    "\n",
    "print(\"\\nCommunity Composition:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for comm_id in sorted(communities_dict.keys()):\n",
    "    regions_in_community = communities_dict[comm_id]\n",
    "    n_regions_in_comm = len(regions_in_community)\n",
    "    \n",
    "    print(f\"\\nCommunity {comm_id}:\")\n",
    "    print(f\"  Size: {n_regions_in_comm} regions\")\n",
    "    print(f\"  Regions: {sorted(regions_in_community)}\")\n",
    "    \n",
    "    # Calculate internal connectivity (density within community)\n",
    "    if n_regions_in_comm > 1:\n",
    "        subgraph = G.subgraph(regions_in_community)\n",
    "        internal_edges = subgraph.number_of_edges()\n",
    "        max_edges = n_regions_in_comm * (n_regions_in_comm - 1) / 2\n",
    "        internal_density = internal_edges / max_edges if max_edges > 0 else 0\n",
    "        print(f\"  Internal density: {internal_density:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate global network properties\n",
    "# These describe the overall organization of the brain connectome\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GLOBAL NETWORK PROPERTIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Basic properties\n",
    "print(f\"\\nBasic Properties:\")\n",
    "print(f\"  Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  Number of edges: {G.number_of_edges()}\")\n",
    "\n",
    "# Density\n",
    "density = nx.density(G)\n",
    "print(f\"\\nDensity:\")\n",
    "print(f\"  {density:.4f}\")\n",
    "print(f\"    Interpretation: {density*100:.2f}% of possible connections exist\")\n",
    "\n",
    "# Average clustering coefficient\n",
    "avg_clustering = nx.average_clustering(G, weight='weight')\n",
    "print(f\"\\nClustering:\")\n",
    "print(f\"  Average clustering coefficient: {avg_clustering:.4f}\")\n",
    "print(f\"    Interpretation: Measure of local cliquishness\")\n",
    "\n",
    "# Connectivity\n",
    "is_connected = nx.is_connected(G)\n",
    "n_components = nx.number_connected_components(G)\n",
    "print(f\"\\nConnectivity:\")\n",
    "print(f\"  Is connected: {is_connected}\")\n",
    "print(f\"  Connected components: {n_components}\")\n",
    "\n",
    "# Path properties (if connected)\n",
    "if is_connected:\n",
    "    avg_path_length = nx.average_shortest_path_length(G, weight='weight')\n",
    "    diameter = nx.diameter(G, weight='weight')\n",
    "    print(f\"\\nPath Properties:\")\n",
    "    print(f\"  Average shortest path: {avg_path_length:.4f}\")\n",
    "    print(f\"  Network diameter: {diameter}\")\n",
    "    print(f\"    Interpretation: Maximum steps needed to reach any node\")\n",
    "\n",
    "# Degree statistics\n",
    "degrees = [G.degree(n) for n in G.nodes()]\n",
    "print(f\"\\nDegree Statistics:\")\n",
    "print(f\"  Mean degree: {np.mean(degrees):.2f}\")\n",
    "print(f\"  Median degree: {np.median(degrees):.2f}\")\n",
    "print(f\"  Min degree: {np.min(degrees)}\")\n",
    "print(f\"  Max degree: {np.max(degrees)}\")\n",
    "\n",
    "# Small-world properties\n",
    "print(f\"\\nNetwork Characteristics:\")\n",
    "print(f\"  Modularity: {modularity:.4f}\")\n",
    "print(f\"  Communities: {n_communities}\")\n",
    "print(f\"\\n  Brain networks typically show:\")\n",
    "print(f\"  - High clustering (modular organization)\")\n",
    "print(f\"  - Low average path length (efficient communication)\")\n",
    "print(f\"  - Multiple communities (functional systems)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare community data for JSON export\n",
    "# JSON is a human-readable format suitable for sharing results\n",
    "\n",
    "communities_for_export = {\n",
    "    'n_communities': n_communities,\n",
    "    'modularity': float(modularity),\n",
    "    'communities': {str(k): v for k, v in communities_dict.items()}\n",
    "}\n",
    "\n",
    "# Test JSON validity\n",
    "try:\n",
    "    json_str = json.dumps(communities_for_export)\n",
    "    print(\"JSON structure is valid\")\nexcept Exception as e:\n",
    "    print(f\"Error creating JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save communities to JSON file\n",
    "# This preserves the community structure for visualization and further analysis\n",
    "\n",
    "communities_output_path = results_dir / 'communities.json'\n",
    "with open(communities_output_path, 'w') as f:\n",
    "    json.dump(communities_for_export, f, indent=2)\n",
    "\n",
    "print(f\"Communities saved to {communities_output_path}\")\n",
    "\n",
    "# Verify the file\n",
    "file_size = communities_output_path.stat().st_size\n",
    "print(f\"File size: {file_size} bytes\")\n",
    "\n",
    "# Verify JSON is valid by reading it back\n",
    "with open(communities_output_path, 'r') as f:\n",
    "    loaded_communities = json.load(f)\n",
    "print(f\"JSON verification: Successfully loaded {loaded_communities['n_communities']} communities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary and visualizations\n",
    "print(\"\\nCreating visualizations...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Brain Network Analysis Summary', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 1: Top 15 hubs by hub score\n",
    "ax = axes[0, 0]\n",
    "top_15_for_plot = top_hubs.head(15).sort_values('hub_score')\n",
    "regions = [f\"R{int(r)}\" for r in top_15_for_plot['region_id']]\n",
    "ax.barh(regions, top_15_for_plot['hub_score'], color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Hub Score (composite centrality)')\n",
    "ax.set_title('Top 15 Hub Regions')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 2: Centrality measures scatter plot\n",
    "ax = axes[0, 1]\n",
    "scatter = ax.scatter(metrics_df['degree'], metrics_df['betweenness'], \n",
    "                     c=metrics_df['hub_score'], cmap='viridis', s=50, alpha=0.6, edgecolors='black')\n",
    "ax.set_xlabel('Degree (number of connections)')\n",
    "ax.set_ylabel('Betweenness (pathway importance)')\n",
    "ax.set_title('Degree vs Betweenness Centrality')\n",
    "plt.colorbar(scatter, ax=ax, label='Hub Score')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Community sizes\n",
    "ax = axes[1, 0]\n",
    "community_sizes = [len(communities_dict[comm_id]) for comm_id in sorted(communities_dict.keys())]\n",
    "colors_comm = plt.cm.Set3(range(n_communities))\n",
    "ax.bar(sorted(communities_dict.keys()), community_sizes, color=colors_comm, alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Community ID')\n",
    "ax.set_ylabel('Number of Regions')\n",
    "ax.set_title(f'Community Structure ({n_communities} communities, modularity={modularity:.3f})')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Summary statistics\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"NETWORK ANALYSIS SUMMARY\n",
    "\n",
    "Centrality Measures (Top hub regions):\n",
    "  Hub Score Range: {metrics_df['hub_score'].min():.4f}-{metrics_df['hub_score'].max():.4f}\n",
    "  Degree Range: {metrics_df['degree'].min():.0f}-{metrics_df['degree'].max():.0f}\n",
    "  Betweenness Range: {metrics_df['betweenness'].min():.4f}-{metrics_df['betweenness'].max():.4f}\n",
    "\n",
    "Community Structure:\n",
    "  Communities: {n_communities}\n",
    "  Modularity: {modularity:.4f}\n",
    "  Sizes: {sorted(community_sizes)}\n",
    "\n",
    "Global Properties:\n",
    "  Density: {density:.4f}\n",
    "  Clustering: {avg_clustering:.4f}\n",
    "  {f'Avg Path: {avg_path_length:.2f}' if is_connected else 'Disconnected'}\n",
    "\n",
    "Interpretation:\n",
    "  Top regions integrate information\n",
    "  Communities = functional systems\n",
    "  High clustering = local processing\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, \n",
    "        fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "analysis_plot_path = figures_dir / 'graph_analysis_summary.png'\n",
    "plt.savefig(analysis_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Analysis visualization saved to {analysis_plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final comprehensive summary\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"GRAPH ANALYSIS SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\nCentrality Analysis:\")\n",
    "print(f\"  Computed metrics: Degree, Betweenness, PageRank, Clustering, Strength\")\n",
    "print(f\"  All metrics normalized to 0-1 scale\")\n",
    "print(f\"  Composite hub score = average of normalized metrics\")\n",
    "print(f\"  Identified {n_top_hubs} top hub regions (see hub_regions.csv)\")\n",
    "\n",
    "print(f\"\\nHub Regions Interpretation:\")\n",
    "print(f\"  - High degree: Many connections (local connectivity)\")\n",
    "print(f\"  - High betweenness: Important for information routing (integration)\")\n",
    "print(f\"  - High pagerank: Connected to other important regions\")\n",
    "print(f\"  - High strength: Strong functional coupling\")\n",
    "print(f\"  - High clustering: Part of tight local modules\")\n",
    "\n",
    "print(f\"\\nCommunity Detection:\")\n",
    "print(f\"  Method: Greedy modularity optimization\")\n",
    "print(f\"  Communities found: {n_communities}\")\n",
    "print(f\"  Modularity score: {modularity:.4f}\")\n",
    "print(f\"  Community sizes: {sorted(community_sizes)}\")\n",
    "\n",
    "print(f\"\\nCommunity Interpretation:\")\n",
    "print(f\"  - Communities = functional systems with strong internal connectivity\")\n",
    "print(f\"  - High modularity = well-defined communities\")\n",
    "print(f\"  - Regions in same community likely work together\")\n",
    "\n",
    "print(f\"\\nGlobal Network Properties:\")\n",
    "print(f\"  Nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  Edges: {G.number_of_edges()}\")\n",
    "print(f\"  Density: {density:.4f}\")\n",
    "print(f\"  Clustering: {avg_clustering:.4f}\")\n",
    "if is_connected:\n",
    "    print(f\"  Avg path length: {avg_path_length:.4f}\")\n",
    "    print(f\"  Diameter: {diameter}\")\n",
    "\n",
    "print(f\"\\nSmall-World Properties:\")\n",
    "print(f\"  High clustering + low path length = efficient brain network\")\n",
    "print(f\"  Modular organization + global integration\")\n",
    "print(f\"  Robust to random perturbations\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  Top hubs: {hub_output_path}\")\n",
    "print(f\"  All metrics: {full_metrics_path}\")\n",
    "print(f\"  Communities: {communities_output_path}\")\n",
    "print(f\"  Visualization: {analysis_plot_path}\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  1. Visualize full connectome and network structure (Notebook 06)\")\n",
    "print(f\"  2. Create publication-quality figures\")\n",
    "print(f\"  3. Compare with other datasets or groups\")\n",
    "print(f\"\\n\" + \"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-application/json",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
